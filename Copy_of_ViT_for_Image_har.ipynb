{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 3733921,
          "sourceType": "datasetVersion",
          "datasetId": 2232355
        }
      ],
      "dockerImageVersionId": 30635,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thenaivekid/Human-action-recognition/blob/main/Copy_of_ViT_for_Image_har.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6Ed6V8V2Ew_",
        "outputId": "f72d18ad-f541-408c-fea5-c656f53ceb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "meetnagadia_human_action_recognition_har_dataset_path = kagglehub.dataset_download('meetnagadia/human-action-recognition-har-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "snB8vRit11EU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff8725f-28c9-459c-8dd3-4cc581068c7b"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/meetnagadia/human-action-recognition-har-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 297M/297M [00:16<00:00, 18.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "meetnagadia_human_action_recognition_har_dataset_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yVEPBX2r4k2-",
        "outputId": "4f85637b-e630-4162-eea6-1e4b2cd67a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/kagglehub/datasets/meetnagadia/human-action-recognition-har-dataset/versions/1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/kagglehub/datasets/meetnagadia/human-action-recognition-har-dataset/versions/1/Human\\ Action\\ Recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYoUipQF3x6v",
        "outputId": "db6a2752-69e0-4ba3-9503-bde1876da466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test  Testing_set.csv  train  Training_set.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up sapiens"
      ],
      "metadata": {
        "id": "AN0ACjyw11Ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1| what is HAR\n",
        "* Human activity recognition, or HAR for short, is a broad field of study concerned with identifying the specific movement or action of a person based on sensor data.\n",
        "* Movements are often typical activities performed indoors, such as walking, talking, standing, and sitting\n",
        "\n",
        "\n",
        "# Why it is important ?\n",
        "* Human activity recognition plays a significant role in human-to-human interaction and interpersonal relations.\n",
        "* Because it provides information about the identity of a person, their personality, and psychological state, it is difficult to extract.\n",
        "* The human ability to recognize another person’s activities is one of the main subjects of study of the scientific areas of computer vision and machine learning. As a result of this research, many applications, including video surveillance systems, human-computer interaction, and robotics for human behavior characterization, require a multiple activity recognition system."
      ],
      "metadata": {
        "id": "T_VrGnZA11Eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dataset\n",
        "12k training images"
      ],
      "metadata": {
        "id": "ASdiUwki11Ew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Best CNN Model  \n",
        "\n",
        "## Model Architecture  \n",
        "| Layer | Type | Output Shape | Parameters |\n",
        "|--------|--------------|----------------|-------------|\n",
        "| **EfficientNetB7** | Functional | (None, 2560) | 64,097,687 |\n",
        "| **Flatten** | Flatten | (None, 2560) | 0 |\n",
        "| **Dense** | Fully Connected | (None, 512) | 1,311,232 |\n",
        "| **Dense_1** | Fully Connected | (None, 15) | 7,695 |\n",
        "\n",
        "---\n",
        "\n",
        "## **Total Parameters:** 65,416,614 (≈ 249.54 MB)  \n",
        "- **Trainable Parameters:** 1,318,927 (≈ 5.03 MB)  \n",
        "- **Non-trainable Parameters:** 64,097,687 (≈ 244.51 MB)  \n",
        "\n"
      ],
      "metadata": {
        "id": "MOjrCN6q11E1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2| Importing libraries"
      ],
      "metadata": {
        "id": "16cRnony11E2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.image as img\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-01T13:17:13.091964Z",
          "iopub.execute_input": "2025-02-01T13:17:13.092665Z",
          "iopub.status.idle": "2025-02-01T13:17:16.862512Z",
          "shell.execute_reply.started": "2025-02-01T13:17:13.092634Z",
          "shell.execute_reply": "2025-02-01T13:17:16.861697Z"
        },
        "id": "bIwp1hxp11E5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df831c26-acab-40dd-f078-2a0b5f7ce64e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cf387009390>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3| Getting the path and Loading the data"
      ],
      "metadata": {
        "id": "mEyGFJRR11E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(f\"{meetnagadia_human_action_recognition_har_dataset_path}/Human Action Recognition/Training_set.csv\")\n",
        "test_data = pd.read_csv(f\"{meetnagadia_human_action_recognition_har_dataset_path}/Human Action Recognition/Testing_set.csv\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:17:16.863917Z",
          "iopub.execute_input": "2025-02-01T13:17:16.864309Z",
          "iopub.status.idle": "2025-02-01T13:17:16.92053Z",
          "shell.execute_reply.started": "2025-02-01T13:17:16.864284Z",
          "shell.execute_reply": "2025-02-01T13:17:16.919583Z"
        },
        "trusted": true,
        "id": "b8NoxepG11FA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-01T13:17:16.921654Z",
          "iopub.execute_input": "2025-02-01T13:17:16.921878Z",
          "iopub.status.idle": "2025-02-01T13:17:16.93459Z",
          "shell.execute_reply.started": "2025-02-01T13:17:16.921859Z",
          "shell.execute_reply": "2025-02-01T13:17:16.933669Z"
        },
        "id": "7A6Yfy2311FD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "7217df73-297e-43f2-9c72-4a7139da6876"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      filename\n",
              "0  Image_1.jpg\n",
              "1  Image_2.jpg\n",
              "2  Image_3.jpg\n",
              "3  Image_4.jpg\n",
              "4  Image_5.jpg"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d14ee47d-be5f-49f4-b184-67d9b6a925c8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Image_1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Image_2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Image_3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Image_4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Image_5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d14ee47d-be5f-49f4-b184-67d9b6a925c8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d14ee47d-be5f-49f4-b184-67d9b6a925c8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d14ee47d-be5f-49f4-b184-67d9b6a925c8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-536b4c95-b4f5-4419-be5b-ec61ea9559cc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-536b4c95-b4f5-4419-be5b-ec61ea9559cc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-536b4c95-b4f5-4419-be5b-ec61ea9559cc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_data",
              "summary": "{\n  \"name\": \"test_data\",\n  \"rows\": 5400,\n  \"fields\": [\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5400,\n        \"samples\": [\n          \"Image_2477.jpg\",\n          \"Image_1048.jpg\",\n          \"Image_3165.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "labels = train_data.label.unique()\n",
        "label2idx = {label: idx for idx, label in enumerate(labels)}\n",
        "idx2label = {idx: label for idx, label in enumerate(labels)}\n",
        "label2idx"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-01T13:17:16.936879Z",
          "iopub.execute_input": "2025-02-01T13:17:16.937141Z",
          "iopub.status.idle": "2025-02-01T13:17:16.946058Z",
          "shell.execute_reply.started": "2025-02-01T13:17:16.937093Z",
          "shell.execute_reply": "2025-02-01T13:17:16.945229Z"
        },
        "id": "aiugG_or11FF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d11a698-f99d-48da-832c-d7a0c74b139c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sitting': 0,\n",
              " 'using_laptop': 1,\n",
              " 'hugging': 2,\n",
              " 'sleeping': 3,\n",
              " 'drinking': 4,\n",
              " 'clapping': 5,\n",
              " 'dancing': 6,\n",
              " 'cycling': 7,\n",
              " 'calling': 8,\n",
              " 'laughing': 9,\n",
              " 'eating': 10,\n",
              " 'fighting': 11,\n",
              " 'listening_to_music': 12,\n",
              " 'running': 13,\n",
              " 'texting': 14}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### skip test because it does not have ground truth values"
      ],
      "metadata": {
        "id": "4wq_ngxo11FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.label.value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:17:16.947178Z",
          "iopub.execute_input": "2025-02-01T13:17:16.94744Z",
          "iopub.status.idle": "2025-02-01T13:17:16.96126Z",
          "shell.execute_reply.started": "2025-02-01T13:17:16.947419Z",
          "shell.execute_reply": "2025-02-01T13:17:16.960378Z"
        },
        "trusted": true,
        "id": "O7WQnCyP11FI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "007b8d52-48d1-4262-cff7-cb70d8b901bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "sitting               840\n",
              "using_laptop          840\n",
              "hugging               840\n",
              "sleeping              840\n",
              "drinking              840\n",
              "clapping              840\n",
              "dancing               840\n",
              "cycling               840\n",
              "calling               840\n",
              "laughing              840\n",
              "eating                840\n",
              "fighting              840\n",
              "listening_to_music    840\n",
              "running               840\n",
              "texting               840\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>sitting</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>using_laptop</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hugging</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sleeping</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>drinking</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>clapping</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dancing</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cycling</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>calling</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>laughing</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>eating</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fighting</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>listening_to_music</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>running</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>texting</th>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "47Ji7xFk11FJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from typing import List, Tuple\n",
        "\n",
        "# -----------------------------\n",
        "# Preprocessing functions\n",
        "# -----------------------------\n",
        "def create_preprocessor(input_size: Tuple[int, int],\n",
        "                        mean: List[float] = (0.485, 0.456, 0.406),\n",
        "                        std: List[float] = (0.229, 0.224, 0.225)):\n",
        "    \"\"\"\n",
        "    Basic preprocessing: Resize, convert to tensor, and normalize.\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std),\n",
        "        # Note: The following Lambda unsqueeze is optional.\n",
        "        # DataLoaders already add the batch dimension, so it might not be needed.\n",
        "        # transforms.Lambda(lambda x: x.unsqueeze(0))\n",
        "    ])\n",
        "\n",
        "def create_train_augmentations(input_size: Tuple[int, int],\n",
        "                               mean: List[float] = (0.485, 0.456, 0.406),\n",
        "                               std: List[float] = (0.229, 0.224, 0.225)):\n",
        "    \"\"\"\n",
        "    Preprocessing for training that includes augmentations:\n",
        "      - RandomResizedCrop: scales and crops the image randomly.\n",
        "      - RandomHorizontalFlip: randomly flips the image.\n",
        "      - ColorJitter: applies random photometric distortions.\n",
        "      - Finally converts to tensor and normalizes.\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "    ])\n",
        "\n",
        "# -----------------------------\n",
        "# Custom Dataset Class\n",
        "# -----------------------------\n",
        "class ActionDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, root_dir: str, transform=None, label2idx=label2idx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame with columns \"filename\" and \"label\".\n",
        "            root_dir (str): Directory where the images are stored.\n",
        "            transform: Transformations to apply to each image.\n",
        "        \"\"\"\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.label2idx = label2idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the row from the DataFrame.\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.root_dir, row['filename'])\n",
        "\n",
        "        # Open the image and ensure it is in RGB format.\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Convert the textual label into an integer.\n",
        "        label = self.label2idx[row['label']]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# -----------------------------\n",
        "# Main setup: Split DataFrames and create DataLoaders\n",
        "# -----------------------------\n",
        "\n",
        "# Split train_df into training and validation sets (80% train, 20% val)\n",
        "train_split_df, val_split_df = train_test_split(\n",
        "    train_data,\n",
        "    test_size=0.05,\n",
        "    stratify=train_data['label'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define the input size expected by your model (e.g., Sapiens expects 1024x1024)\n",
        "# input_size = (1024, 1024)\n",
        "input_size = (224,224)\n",
        "\n",
        "# Create transformation pipelines.\n",
        "train_transform = create_train_augmentations(input_size)\n",
        "val_transform = create_preprocessor(input_size)\n",
        "\n",
        "# Set the directory where your images are stored.\n",
        "train_data_root_dir = f\"{meetnagadia_human_action_recognition_har_dataset_path}/Human Action Recognition/train\"  # Replace with your actual image directory.\n",
        "test_data_root_dir = f\"{meetnagadia_human_action_recognition_har_dataset_path}/Human Action Recognition/test\"\n",
        "# Create datasets.\n",
        "train_dataset = ActionDataset(train_split_df, train_data_root_dir, transform=train_transform)\n",
        "val_dataset = ActionDataset(val_split_df, train_data_root_dir, transform=val_transform)\n",
        "# test_dataset = ActionDataset(test_data, test_data_root_dir, transform=val_transform)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-01T13:17:16.962569Z",
          "iopub.execute_input": "2025-02-01T13:17:16.963026Z",
          "iopub.status.idle": "2025-02-01T13:17:17.494917Z",
          "shell.execute_reply.started": "2025-02-01T13:17:16.962967Z",
          "shell.execute_reply": "2025-02-01T13:17:17.494255Z"
        },
        "id": "aTDN_8X411FK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# finetuning SAPIENS\n"
      ],
      "metadata": {
        "id": "_jT3vkov11FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # !wget https://huggingface.co/facebook/sapiens-pretrain-0.3b/resolve/main/sapiens_0.3b_epoch_1600_clean.pth\n",
        "# !wget https://huggingface.co/facebook/sapiens-pretrain-0.3b-torchscript/resolve/main/sapiens_0.3b_epoch_1600_torchscript.pt2\n",
        "\n",
        "# !wget -nc https://learnopencv.com/wp-content/uploads/2024/09/man-horse-arrow-scaled.jpg -O man-horse-arrow.jpg\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-01T13:17:17.495771Z",
          "iopub.execute_input": "2025-02-01T13:17:17.496024Z",
          "iopub.status.idle": "2025-02-01T13:17:17.499645Z",
          "shell.execute_reply.started": "2025-02-01T13:17:17.496002Z",
          "shell.execute_reply": "2025-02-01T13:17:17.498728Z"
        },
        "id": "Yy0F88kk11FO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class ImageActionClassifier(nn.Module):\n",
        "    def __init__(self, sapiens_model, num_classes):\n",
        "        super(ImageActionClassifier, self).__init__()\n",
        "        self.sapiens_model = sapiens_model\n",
        "        for param in self.sapiens_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.classifier_head = nn.Sequential(\n",
        "\n",
        "            nn.Linear(768, 1024),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.Dropout(p=0.4),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = self.sapiens_model(x)\n",
        "        return self.classifier_head(x.last_hidden_state[:,0,:])\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-01T13:25:30.65976Z",
          "iopub.execute_input": "2025-02-01T13:25:30.660656Z",
          "iopub.status.idle": "2025-02-01T13:25:30.668237Z",
          "shell.execute_reply.started": "2025-02-01T13:25:30.660624Z",
          "shell.execute_reply": "2025-02-01T13:25:30.667027Z"
        },
        "id": "VGP35bsz11FP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# sapiens_model_path = \"sapiens_0.3b_epoch_1600_torchscript.pt2\"\n",
        "\n",
        "# sapiens_model = torch.jit.load(sapiens_model_path)\n",
        "\n",
        "from transformers import ViTModel\n",
        "sapiens_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "sapiens_model.eval()\n",
        "\n",
        "\n",
        "model = ImageActionClassifier(sapiens_model, 15)\n",
        "\n",
        "new_state_dict = {}\n",
        "old_checkpoint=torch.load(\"/content/drive/MyDrive/har/vit-backbone-image-action-epoch4-lre-4_schedule_run4.pt\")\n",
        "for key, value in old_checkpoint.items():\n",
        "    if key.startswith(\"classifier_head.\"):\n",
        "        # Split the key into parts (e.g., \"classifier_head.1.weight\")\n",
        "        parts = key.split(\".\")\n",
        "        layer_idx = int(parts[1])\n",
        "        # Map the old indices to new indices\n",
        "        if layer_idx == 0:\n",
        "            new_layer_idx = 0  # first linear layer remains at index 0\n",
        "        elif layer_idx == 1:\n",
        "            new_layer_idx = 2  # second linear layer moves from index 1 to index 2\n",
        "        elif layer_idx == 2:\n",
        "            new_layer_idx = 4  # third linear layer moves from index 2 to index 4\n",
        "        else:\n",
        "            # In case there's something unexpected, keep the same key.\n",
        "            new_layer_idx = layer_idx\n",
        "\n",
        "        # Rebuild the key with the new index.\n",
        "        new_key = f\"classifier_head.{new_layer_idx}.\" + \".\".join(parts[2:])\n",
        "        new_state_dict[new_key] = value\n",
        "    else:\n",
        "        # For keys not in classifier_head, keep them as is.\n",
        "        new_state_dict[key] = value\n",
        "\n",
        "\n",
        "\n",
        "model.load_state_dict(new_state_dict)\n",
        "# model(torch.rand(size=(1,3,1024, 1024)))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel.\")\n",
        "    model = torch.nn.DataParallel(model)\n",
        "model = model.to(device)\n",
        "print(\"device \", device)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-01T13:25:32.948736Z",
          "iopub.execute_input": "2025-02-01T13:25:32.949067Z",
          "iopub.status.idle": "2025-02-01T13:25:33.397285Z",
          "shell.execute_reply.started": "2025-02-01T13:25:32.949043Z",
          "shell.execute_reply": "2025-02-01T13:25:33.396296Z"
        },
        "id": "VmW8_oWU11FQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a4cb6d-2037-455b-f350-9fd39d7ac30f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-8f6d90947e65>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  old_checkpoint=torch.load(\"/content/drive/MyDrive/har/vit-backbone-image-action-epoch4-lre-4_schedule_run4.pt\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device  cuda\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "num_parameters = sum(p.numel() for p in model.parameters() )\n",
        "print(f\"Number of parameters: {num_parameters}\")\n",
        "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_parameters}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-01T13:17:22.259537Z",
          "iopub.execute_input": "2025-02-01T13:17:22.25996Z",
          "iopub.status.idle": "2025-02-01T13:17:22.267019Z",
          "shell.execute_reply.started": "2025-02-01T13:17:22.259934Z",
          "shell.execute_reply": "2025-02-01T13:17:22.265975Z"
        },
        "id": "lOlRCyIj11FS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e8eee9-8d23-457e-c734-5d9533660281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 87709199\n",
            "Number of trainable parameters: 1319951\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "api_key = \"330a4a0723c3988c8d367cbb822d3d6624621fbd\"\n",
        "wandb.login(key=api_key)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-01T13:17:22.268044Z",
          "iopub.execute_input": "2025-02-01T13:17:22.268366Z",
          "iopub.status.idle": "2025-02-01T13:17:24.761813Z",
          "shell.execute_reply.started": "2025-02-01T13:17:22.268328Z",
          "shell.execute_reply": "2025-02-01T13:17:24.760943Z"
        },
        "id": "C864QbKN11FT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1012533-eee8-4644-ed64-1f563c5fd2d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-01T13:17:24.762877Z",
          "iopub.execute_input": "2025-02-01T13:17:24.763326Z",
          "iopub.status.idle": "2025-02-01T13:17:24.76796Z",
          "shell.execute_reply.started": "2025-02-01T13:17:24.763301Z",
          "shell.execute_reply": "2025-02-01T13:17:24.766997Z"
        },
        "id": "tXO3onH411FT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab30c35b-4613-41e3-d84e-a9ea800abb62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "\n",
        "name = \"vit-backbone-image-action-epoch4-lre-4_schedule_run5\"\n",
        "wandb.init(project=\"HAR\", name=f\"{name}\")\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "best_val_loss = float('inf')\n",
        "start = time.time()\n",
        "global_step = 0  # Add a global step counter\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=epochs, eta_min=0.0)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    # Training loop\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        pixel_values = batch[0].to(device)\n",
        "        labels = batch[1].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(pixel_values)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient norm calculation\n",
        "        total_norm = 0\n",
        "        for param in model.parameters():\n",
        "            if param.grad is not None:\n",
        "                param_norm = param.grad.data.norm(2)\n",
        "                total_norm += param_norm.item() ** 2\n",
        "        total_norm = total_norm ** 0.5\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        # Compute batch accuracy\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        batch_correct = (preds == labels).sum().item()\n",
        "        batch_total = labels.size(0)\n",
        "        batch_accuracy = batch_correct / batch_total\n",
        "\n",
        "        # Log every step\n",
        "        wandb.log({\n",
        "            \"Step\": global_step,\n",
        "            \"Step Train Loss\": loss.item(),\n",
        "            \"Step Train Accuracy\": batch_accuracy,\n",
        "            \"Step Gradient Norm\": total_norm,\n",
        "            \"Learning Rate\": optim.param_groups[0]['lr'],\n",
        "            \"GPU Memory\": torch.cuda.memory_allocated(device) / (1024 * 1024)\n",
        "        })\n",
        "        print({\n",
        "            \"Step\": global_step,\n",
        "            \"Step Train Loss\": loss.item(),\n",
        "            \"Step Train Accuracy\": batch_accuracy,\n",
        "            \"Step Gradient Norm\": total_norm,\n",
        "            \"Learning Rate\": optim.param_groups[0]['lr'],\n",
        "            \"GPU Memory\": torch.cuda.memory_allocated(device) / (1024 * 1024)\n",
        "        })\n",
        "        # Accumulate metrics for epoch-level logging\n",
        "        train_loss += loss.item() * batch_total\n",
        "        train_correct += batch_correct\n",
        "        train_total += batch_total\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "        if batch_idx % 100 == 0:  # Print every 100 steps\n",
        "            print(f\"Epoch {epoch+1}, Step {batch_idx}, \"\n",
        "                  f\"Step Loss: {loss.item():.4f}, \"\n",
        "                  f\"Step Accuracy: {batch_accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "    # Calculate epoch-level metrics\n",
        "    epoch_train_loss = train_loss / train_total\n",
        "    epoch_train_accuracy = train_correct / train_total\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            pixel_values = batch[0].to(device)\n",
        "            labels = batch[1].to(device)\n",
        "\n",
        "            outputs = model(pixel_values)\n",
        "            batch_loss = criterion(outputs, labels).item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "            val_loss += batch_loss * labels.size(0)\n",
        "\n",
        "    # Calculate and log epoch-level validation metrics\n",
        "    epoch_val_loss = val_loss / val_total\n",
        "    epoch_val_accuracy = val_correct / val_total\n",
        "    scheduler.step()\n",
        "    # Log epoch-level metrics\n",
        "    wandb.log({\n",
        "        \"Epoch\": epoch,\n",
        "        \"Epoch Train Loss\": epoch_train_loss,\n",
        "        \"Epoch Train Accuracy\": epoch_train_accuracy,\n",
        "        \"Epoch Validation Loss\": epoch_val_loss,\n",
        "        \"Epoch Validation Accuracy\": epoch_val_accuracy,\n",
        "    })\n",
        "\n",
        "    # Save best model\n",
        "    if epoch_val_loss < best_val_loss:\n",
        "        best_val_loss = epoch_val_loss\n",
        "        save_path = \"/content/drive/MyDrive/har\"\n",
        "        torch.save(\n",
        "            model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict(),\n",
        "            f\"{save_path}/{name}.pt\"\n",
        "        )\n",
        "        print(f\"Saved checkpoint at {save_path}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "          f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
        "          f\"Train Accuracy: {epoch_train_accuracy*100:.2f}%, \"\n",
        "          f\"Val Loss: {epoch_val_loss:.4f}, \"\n",
        "          f\"Val Accuracy: {epoch_val_accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "wandb.finish()\n",
        "df = pd.DataFrame({\"time taken\": (time.time() - start)/60, \"epochs\": epochs}, index=[0])\n",
        "df.to_csv(f\"/content/drive/MyDrive/har/time_{name}.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-01T13:28:44.897785Z",
          "iopub.execute_input": "2025-02-01T13:28:44.898157Z",
          "iopub.status.idle": "2025-02-01T13:28:52.464079Z",
          "shell.execute_reply.started": "2025-02-01T13:28:44.898108Z",
          "shell.execute_reply": "2025-02-01T13:28:52.462716Z"
        },
        "id": "y6vFj2uF11FU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "348ffd17-3494-48ef-95ba-ea5b0dea3f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Step': 0, 'Step Train Loss': 0.44418951869010925, 'Step Train Accuracy': 0.8515625, 'Step Gradient Norm': 1.0642152033056644, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "Epoch 1, Step 0, Step Loss: 0.4442, Step Accuracy: 85.16%\n",
            "{'Step': 1, 'Step Train Loss': 0.5356581211090088, 'Step Train Accuracy': 0.8046875, 'Step Gradient Norm': 1.2843651056208565, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 2, 'Step Train Loss': 0.4134218692779541, 'Step Train Accuracy': 0.8359375, 'Step Gradient Norm': 1.1294821224720613, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 3, 'Step Train Loss': 0.6171606779098511, 'Step Train Accuracy': 0.796875, 'Step Gradient Norm': 1.2189589513628245, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 4, 'Step Train Loss': 0.46908146142959595, 'Step Train Accuracy': 0.828125, 'Step Gradient Norm': 1.1514562696193653, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 5, 'Step Train Loss': 0.32792410254478455, 'Step Train Accuracy': 0.90625, 'Step Gradient Norm': 0.9123127046652285, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 6, 'Step Train Loss': 0.5954362154006958, 'Step Train Accuracy': 0.8125, 'Step Gradient Norm': 1.3851414987994983, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 7, 'Step Train Loss': 0.6183565855026245, 'Step Train Accuracy': 0.8203125, 'Step Gradient Norm': 1.2128481345592264, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 8, 'Step Train Loss': 0.5266911387443542, 'Step Train Accuracy': 0.8359375, 'Step Gradient Norm': 1.1128474074157462, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 9, 'Step Train Loss': 0.6560714840888977, 'Step Train Accuracy': 0.765625, 'Step Gradient Norm': 1.5118169918049373, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 10, 'Step Train Loss': 0.5827674269676208, 'Step Train Accuracy': 0.8046875, 'Step Gradient Norm': 1.3293631725033854, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 11, 'Step Train Loss': 0.6877850890159607, 'Step Train Accuracy': 0.75, 'Step Gradient Norm': 1.60113122512254, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 12, 'Step Train Loss': 0.46292370557785034, 'Step Train Accuracy': 0.8359375, 'Step Gradient Norm': 1.2076452279267536, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 13, 'Step Train Loss': 0.5816364288330078, 'Step Train Accuracy': 0.8125, 'Step Gradient Norm': 1.3744768700455432, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 14, 'Step Train Loss': 0.6652718186378479, 'Step Train Accuracy': 0.7734375, 'Step Gradient Norm': 1.4716032962543089, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 15, 'Step Train Loss': 0.6546287536621094, 'Step Train Accuracy': 0.78125, 'Step Gradient Norm': 1.5621802947016292, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 16, 'Step Train Loss': 0.5902450084686279, 'Step Train Accuracy': 0.8203125, 'Step Gradient Norm': 1.4868430166418016, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 17, 'Step Train Loss': 0.41069966554641724, 'Step Train Accuracy': 0.875, 'Step Gradient Norm': 1.0591788355721667, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 18, 'Step Train Loss': 0.5192545652389526, 'Step Train Accuracy': 0.8046875, 'Step Gradient Norm': 1.3816931690988719, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 19, 'Step Train Loss': 0.4933038651943207, 'Step Train Accuracy': 0.8515625, 'Step Gradient Norm': 1.3412129178156822, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 20, 'Step Train Loss': 0.7138291001319885, 'Step Train Accuracy': 0.7734375, 'Step Gradient Norm': 1.6119922392805286, 'Learning Rate': 0.001, 'GPU Memory': 1110.74658203125}\n",
            "{'Step': 21, 'Step Train Loss': 0.9002886414527893, 'Step Train Accuracy': 0.75, 'Step Gradient Norm': 1.510986421453876, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 22, 'Step Train Loss': 0.5694443583488464, 'Step Train Accuracy': 0.8203125, 'Step Gradient Norm': 1.435096815186635, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 23, 'Step Train Loss': 0.7468628883361816, 'Step Train Accuracy': 0.7890625, 'Step Gradient Norm': 1.5127840230887881, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 24, 'Step Train Loss': 0.7654556035995483, 'Step Train Accuracy': 0.75, 'Step Gradient Norm': 1.6042293837219392, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 25, 'Step Train Loss': 0.7828181982040405, 'Step Train Accuracy': 0.7578125, 'Step Gradient Norm': 1.5051082924700574, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 26, 'Step Train Loss': 0.5841258764266968, 'Step Train Accuracy': 0.828125, 'Step Gradient Norm': 1.2885520121500833, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 27, 'Step Train Loss': 0.4483759105205536, 'Step Train Accuracy': 0.8515625, 'Step Gradient Norm': 1.1762323317597996, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 28, 'Step Train Loss': 0.651678740978241, 'Step Train Accuracy': 0.8046875, 'Step Gradient Norm': 1.3972058280081117, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 29, 'Step Train Loss': 0.6077424883842468, 'Step Train Accuracy': 0.7890625, 'Step Gradient Norm': 1.2832369846906257, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 30, 'Step Train Loss': 0.6736685037612915, 'Step Train Accuracy': 0.734375, 'Step Gradient Norm': 1.498905469510711, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 31, 'Step Train Loss': 0.4943782091140747, 'Step Train Accuracy': 0.8359375, 'Step Gradient Norm': 1.2469235573550337, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 32, 'Step Train Loss': 0.6715338826179504, 'Step Train Accuracy': 0.7890625, 'Step Gradient Norm': 1.6907681445053635, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 33, 'Step Train Loss': 0.5761231780052185, 'Step Train Accuracy': 0.8125, 'Step Gradient Norm': 1.4638968959954664, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 34, 'Step Train Loss': 0.616047203540802, 'Step Train Accuracy': 0.8046875, 'Step Gradient Norm': 1.3696948210591977, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 35, 'Step Train Loss': 0.5049569010734558, 'Step Train Accuracy': 0.8515625, 'Step Gradient Norm': 1.1169109616819437, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 36, 'Step Train Loss': 0.6486262679100037, 'Step Train Accuracy': 0.78125, 'Step Gradient Norm': 1.4379521803635245, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 37, 'Step Train Loss': 0.6767764091491699, 'Step Train Accuracy': 0.8125, 'Step Gradient Norm': 1.4700915150363014, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 38, 'Step Train Loss': 0.43263232707977295, 'Step Train Accuracy': 0.875, 'Step Gradient Norm': 1.0998769701066828, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 39, 'Step Train Loss': 0.6022431254386902, 'Step Train Accuracy': 0.828125, 'Step Gradient Norm': 1.329256364447058, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 40, 'Step Train Loss': 0.7644888758659363, 'Step Train Accuracy': 0.7578125, 'Step Gradient Norm': 1.7079362671779585, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 41, 'Step Train Loss': 0.5706527233123779, 'Step Train Accuracy': 0.828125, 'Step Gradient Norm': 1.447348910763855, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 42, 'Step Train Loss': 0.5202624797821045, 'Step Train Accuracy': 0.84375, 'Step Gradient Norm': 1.173693047594707, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 43, 'Step Train Loss': 0.44723910093307495, 'Step Train Accuracy': 0.828125, 'Step Gradient Norm': 1.2929352084833396, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 44, 'Step Train Loss': 0.6606929302215576, 'Step Train Accuracy': 0.828125, 'Step Gradient Norm': 1.2032183915844765, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 45, 'Step Train Loss': 0.7487527132034302, 'Step Train Accuracy': 0.7734375, 'Step Gradient Norm': 1.6372059877004412, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 46, 'Step Train Loss': 0.49654844403266907, 'Step Train Accuracy': 0.8125, 'Step Gradient Norm': 1.2298867460346963, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 47, 'Step Train Loss': 0.806286096572876, 'Step Train Accuracy': 0.75, 'Step Gradient Norm': 1.7829550747592258, 'Learning Rate': 0.001, 'GPU Memory': 1110.74560546875}\n",
            "{'Step': 48, 'Step Train Loss': 0.7240996360778809, 'Step Train Accuracy': 0.7734375, 'Step Gradient Norm': 1.4345338016920979, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 49, 'Step Train Loss': 0.5838172435760498, 'Step Train Accuracy': 0.8046875, 'Step Gradient Norm': 1.4489538654359289, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 50, 'Step Train Loss': 0.6937292218208313, 'Step Train Accuracy': 0.7734375, 'Step Gradient Norm': 1.380814445278651, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 51, 'Step Train Loss': 0.7092670202255249, 'Step Train Accuracy': 0.78125, 'Step Gradient Norm': 1.2990647193833678, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 52, 'Step Train Loss': 0.7098326683044434, 'Step Train Accuracy': 0.7578125, 'Step Gradient Norm': 1.50890487973225, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 53, 'Step Train Loss': 0.6055000424385071, 'Step Train Accuracy': 0.8125, 'Step Gradient Norm': 1.2976842297576288, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 54, 'Step Train Loss': 0.5769398808479309, 'Step Train Accuracy': 0.8125, 'Step Gradient Norm': 1.3266916585188402, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 55, 'Step Train Loss': 0.7386035323143005, 'Step Train Accuracy': 0.765625, 'Step Gradient Norm': 1.6081140285893327, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 56, 'Step Train Loss': 0.5346663594245911, 'Step Train Accuracy': 0.8046875, 'Step Gradient Norm': 1.4488387217533971, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 57, 'Step Train Loss': 0.596337616443634, 'Step Train Accuracy': 0.8203125, 'Step Gradient Norm': 1.2957062825296466, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 58, 'Step Train Loss': 0.6140140891075134, 'Step Train Accuracy': 0.8125, 'Step Gradient Norm': 1.2366873846514255, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 59, 'Step Train Loss': 0.5385311841964722, 'Step Train Accuracy': 0.8046875, 'Step Gradient Norm': 1.2961233735495805, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n",
            "{'Step': 60, 'Step Train Loss': 0.6461537480354309, 'Step Train Accuracy': 0.8125, 'Step Gradient Norm': 1.3313214260143467, 'Learning Rate': 0.001, 'GPU Memory': 1110.7470703125}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-a0e0842c59b1>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "AhOnF9S-11FX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import pandas as pd\n",
        "# from collections import defaultdict\n",
        "\n",
        "# # Ensure the model is in evaluation mode\n",
        "# model.eval()\n",
        "\n",
        "# all_preds = []\n",
        "# all_labels = []\n",
        "\n",
        "# # Evaluate the model on the test set.\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         pixel_values = batch[0].to(device)\n",
        "#         labels = batch[1].to(device)\n",
        "\n",
        "#         outputs = model(pixel_values)\n",
        "#         preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "#         all_preds.append(preds.cpu())\n",
        "#         all_labels.append(labels.cpu())\n",
        "#         break\n",
        "\n",
        "# # Concatenate all predictions and labels from the test set\n",
        "# all_preds = torch.cat(all_preds)\n",
        "# all_labels = torch.cat(all_labels)\n",
        "\n",
        "# # Compute total accuracy.\n",
        "# total_accuracy = (all_preds == all_labels).float().mean().item()\n",
        "\n",
        "# # Determine the number of classes.\n",
        "# # Here, we assume your test dataset (or training dataset) has an attribute 'labels'\n",
        "# # that is a sorted list of class names.\n",
        "# num_classes = len(test_dataset.labels)  # Adjust if your dataset stores this differently\n",
        "\n",
        "# # Initialize counters for per-class accuracy.\n",
        "# class_correct = defaultdict(int)\n",
        "# class_total = defaultdict(int)\n",
        "\n",
        "# # Compute per-class correct predictions.\n",
        "# for true_label, pred in zip(all_labels, all_preds):\n",
        "#     true_label = true_label.item()\n",
        "#     pred = pred.item()\n",
        "#     class_total[true_label] += 1\n",
        "#     if true_label == pred:\n",
        "#         class_correct[true_label] += 1\n",
        "\n",
        "# # Prepare data for CSV output.\n",
        "# results = []\n",
        "# for cls in range(num_classes):\n",
        "#     # Get the class name from the dataset; if you don't have names, you can simply use the class index.\n",
        "#     class_name = test_dataset.labels[cls] if hasattr(test_dataset, 'labels') else str(cls)\n",
        "#     total = class_total[cls]\n",
        "#     accuracy = class_correct[cls] / total if total > 0 else 0\n",
        "#     results.append({\n",
        "#         'Class': class_name,\n",
        "#         'Total Samples': total,\n",
        "#         'Accuracy': accuracy\n",
        "#     })\n",
        "\n",
        "# # Add an overall accuracy row.\n",
        "# results.append({\n",
        "#     'Class': 'Overall',\n",
        "#     'Total Samples': len(all_labels),\n",
        "#     'Accuracy': total_accuracy\n",
        "# })\n",
        "\n",
        "# # Create a DataFrame and save it to CSV.\n",
        "# df_results = pd.DataFrame(results)\n",
        "# csv_output_path = 'test_accuracy.csv'\n",
        "# df_results.to_csv(csv_output_path, index=False)\n",
        "\n",
        "# print(\"Test results saved to\", csv_output_path)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.365334Z",
          "iopub.status.idle": "2025-02-01T13:18:06.365819Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.36557Z",
          "shell.execute_reply": "2025-02-01T13:18:06.365593Z"
        },
        "id": "13l01nnx11FY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4| Making function that take random path and display the image"
      ],
      "metadata": {
        "id": "8uSgdiaV11FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def displaying_random_images():\n",
        "#     num = random.randint(1,10000)\n",
        "#     imgg = \"Image_{}.jpg\".format(num)\n",
        "#     train = f\"meetnagadia_human_action_recognition_har_dataset_path/Human Action Recognition/train/\"\n",
        "#     if os.path.exists(train+imgg):\n",
        "#         testImage = img.imread(train+imgg)\n",
        "#         plt.imshow(testImage)\n",
        "#         plt.title(\"{}\".format(train_data.loc[train_data['filename'] == \"{}\".format(imgg), 'label'].item()))\n",
        "\n",
        "#     else:\n",
        "#         #print(train+img)\n",
        "#         print(\"File Path not found \\nSkipping the file!!\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.367162Z",
          "iopub.status.idle": "2025-02-01T13:18:06.367632Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.36739Z",
          "shell.execute_reply": "2025-02-01T13:18:06.367415Z"
        },
        "trusted": true,
        "id": "lmrYXuhA11FZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying_random_images()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.368923Z",
          "iopub.status.idle": "2025-02-01T13:18:06.369367Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.369146Z",
          "shell.execute_reply": "2025-02-01T13:18:06.369168Z"
        },
        "trusted": true,
        "id": "NbAJEb5O11Fa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying_random_images()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.371192Z",
          "iopub.status.idle": "2025-02-01T13:18:06.371602Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.371389Z",
          "shell.execute_reply": "2025-02-01T13:18:06.37141Z"
        },
        "trusted": true,
        "id": "6Ktpl0_U11Fb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying_random_images()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.372827Z",
          "iopub.status.idle": "2025-02-01T13:18:06.373262Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.373024Z",
          "shell.execute_reply": "2025-02-01T13:18:06.373045Z"
        },
        "trusted": true,
        "id": "-rKOWlMv11Fc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying_random_images()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.374724Z",
          "iopub.status.idle": "2025-02-01T13:18:06.375165Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.374922Z",
          "shell.execute_reply": "2025-02-01T13:18:06.374943Z"
        },
        "trusted": true,
        "id": "s3qGpc9V11Fd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5| Data preprocessing"
      ],
      "metadata": {
        "id": "bBZAlQ7811Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# img_data = []\n",
        "# img_label = []\n",
        "# length = len(train_fol)\n",
        "# for i in (range(len(train_fol)-1)):\n",
        "#     t = '../input/human-action-recognition-har-dataset/Human Action Recognition/train/' + filename[i]\n",
        "#     temp_img = Image.open(t)\n",
        "#     img_data.append(np.asarray(temp_img.resize((160,160))))\n",
        "#     img_label.append(situation[i])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.376344Z",
          "iopub.status.idle": "2025-02-01T13:18:06.376633Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.376492Z",
          "shell.execute_reply": "2025-02-01T13:18:06.376506Z"
        },
        "trusted": true,
        "id": "LPLC7HU511Ff"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# img_shape= (160,160,3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.377886Z",
          "iopub.status.idle": "2025-02-01T13:18:06.378206Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.378039Z",
          "shell.execute_reply": "2025-02-01T13:18:06.378053Z"
        },
        "trusted": true,
        "id": "_f-Ob_Jo11Fg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# iii = img_data\n",
        "# iii = np.asarray(iii)\n",
        "# type(iii)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.379576Z",
          "iopub.status.idle": "2025-02-01T13:18:06.379858Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.379724Z",
          "shell.execute_reply": "2025-02-01T13:18:06.379738Z"
        },
        "trusted": true,
        "id": "zIFTWgK111Fg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train = to_categorical(np.asarray(train_data[\"label\"].factorize()[0]))\n",
        "# print(y_train[0])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.380662Z",
          "iopub.status.idle": "2025-02-01T13:18:06.380939Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.380802Z",
          "shell.execute_reply": "2025-02-01T13:18:06.380816Z"
        },
        "trusted": true,
        "id": "Xg_s3OmG11Fh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6| Make an CNN model"
      ],
      "metadata": {
        "id": "BxhZt8mj11Fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# efficientnet_model = Sequential()\n",
        "\n",
        "# model = tf.keras.applications.EfficientNetB7(include_top=False,\n",
        "#                                             input_shape=(160,160,3),\n",
        "#                                             pooling =\"avg\",classes=15,\n",
        "#                                              weights=\"imagenet\")\n",
        "\n",
        "# for layer in model.layers:\n",
        "#     layer.trainable=False\n",
        "\n",
        "\n",
        "# efficientnet_model.add(model)\n",
        "# efficientnet_model.add(Flatten())\n",
        "# efficientnet_model.add(Dense(512,activation=\"relu\"))\n",
        "# efficientnet_model.add(Dense(15,activation=\"softmax\"))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.382274Z",
          "iopub.status.idle": "2025-02-01T13:18:06.382685Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.382476Z",
          "shell.execute_reply": "2025-02-01T13:18:06.382496Z"
        },
        "trusted": true,
        "id": "RW34tsi711Fi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# efficientnet_model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.384348Z",
          "iopub.status.idle": "2025-02-01T13:18:06.38461Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.384484Z",
          "shell.execute_reply": "2025-02-01T13:18:06.384496Z"
        },
        "trusted": true,
        "id": "6q1SKX_g11Fj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# efficientnet_model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.385796Z",
          "iopub.status.idle": "2025-02-01T13:18:06.386099Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.385956Z",
          "shell.execute_reply": "2025-02-01T13:18:06.385971Z"
        },
        "trusted": true,
        "id": "CPWzrBO911F0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# history = efficientnet_model.fit(iii,y_train,epochs=40)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.387174Z",
          "iopub.status.idle": "2025-02-01T13:18:06.387459Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.387318Z",
          "shell.execute_reply": "2025-02-01T13:18:06.387332Z"
        },
        "trusted": true,
        "id": "i8XqvxTp11F0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# losses = history.history[\"loss\"]\n",
        "# plt.plot(losses)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.388907Z",
          "iopub.status.idle": "2025-02-01T13:18:06.389194Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.389039Z",
          "shell.execute_reply": "2025-02-01T13:18:06.389052Z"
        },
        "trusted": true,
        "id": "KChUDWkt11F1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# acc = history.history['accuracy']\n",
        "# plt.plot(acc)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.390279Z",
          "iopub.status.idle": "2025-02-01T13:18:06.390555Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.39042Z",
          "shell.execute_reply": "2025-02-01T13:18:06.390434Z"
        },
        "trusted": true,
        "id": "LOg1bt7Y11F1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7| Model predictions"
      ],
      "metadata": {
        "id": "HIWlDUq011F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def read_img(fn):\n",
        "#     img = Image.open(fn)\n",
        "#     return np.asarray(img.resize((160,160)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.39144Z",
          "iopub.status.idle": "2025-02-01T13:18:06.391695Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.391569Z",
          "shell.execute_reply": "2025-02-01T13:18:06.391581Z"
        },
        "trusted": true,
        "id": "ZPNdCav211F4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# def test_predict(test_image):\n",
        "#     result = efficientnet_model.predict(np.asarray([read_img(test_image)]))\n",
        "\n",
        "#     itemindex = np.where(result==np.max(result))\n",
        "#     prediction = itemindex[1][0]\n",
        "#     print(\"probability: \"+str(np.max(result)*100) + \"%\\nPredicted class : \", prediction)\n",
        "\n",
        "#     image = img.imread(test_image)\n",
        "#     plt.imshow(image)\n",
        "#     plt.title(prediction)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.392903Z",
          "iopub.status.idle": "2025-02-01T13:18:06.393348Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.393104Z",
          "shell.execute_reply": "2025-02-01T13:18:06.39315Z"
        },
        "trusted": true,
        "id": "unq1EGr611F5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# test_predict(f\"meetnagadia_human_action_recognition_har_dataset_path/Human Action Recognition/test/Image_1001.jpg\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.394628Z",
          "iopub.status.idle": "2025-02-01T13:18:06.394899Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.394765Z",
          "shell.execute_reply": "2025-02-01T13:18:06.394778Z"
        },
        "trusted": true,
        "id": "ZppcTXrH11F5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# test_predict(f\"meetnagadia_human_action_recognition_har_dataset_path/Human Action Recognition/test/Image_101.jpg\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.395719Z",
          "iopub.status.idle": "2025-02-01T13:18:06.39598Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.395848Z",
          "shell.execute_reply": "2025-02-01T13:18:06.395866Z"
        },
        "trusted": true,
        "id": "k1_RUEPa11F7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# test_predict(f\"meetnagadia_human_action_recognition_har_dataset_path/Human Action Recognition/test/Image_1056.jpg\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.397667Z",
          "iopub.status.idle": "2025-02-01T13:18:06.397961Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.397807Z",
          "shell.execute_reply": "2025-02-01T13:18:06.39782Z"
        },
        "trusted": true,
        "id": "F9Nr4EZo11F8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# test_predict(f\"meetnagadia_human_action_recognition_har_dataset_path/Human Action Recognition/test/Image_1024.jpg\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-01T13:18:06.39887Z",
          "iopub.status.idle": "2025-02-01T13:18:06.39928Z",
          "shell.execute_reply.started": "2025-02-01T13:18:06.399038Z",
          "shell.execute_reply": "2025-02-01T13:18:06.39906Z"
        },
        "trusted": true,
        "id": "bGOEABbY11F_"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}