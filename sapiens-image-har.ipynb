{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3733921,"sourceType":"datasetVersion","datasetId":2232355}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting up sapiens","metadata":{}},{"cell_type":"markdown","source":"# 1| what is HAR\n* Human activity recognition, or HAR for short, is a broad field of study concerned with identifying the specific movement or action of a person based on sensor data.\n* Movements are often typical activities performed indoors, such as walking, talking, standing, and sitting\n\n\n# Why it is important ?\n* Human activity recognition plays a significant role in human-to-human interaction and interpersonal relations.\n* Because it provides information about the identity of a person, their personality, and psychological state, it is difficult to extract.\n* The human ability to recognize another person’s activities is one of the main subjects of study of the scientific areas of computer vision and machine learning. As a result of this research, many applications, including video surveillance systems, human-computer interaction, and robotics for human behavior characterization, require a multiple activity recognition system.","metadata":{}},{"cell_type":"markdown","source":"# dataset\n12k training images","metadata":{}},{"cell_type":"markdown","source":"\n# Best CNN Model  \n\n## Model Architecture  \n| Layer | Type | Output Shape | Parameters |\n|--------|--------------|----------------|-------------|\n| **EfficientNetB7** | Functional | (None, 2560) | 64,097,687 |\n| **Flatten** | Flatten | (None, 2560) | 0 |\n| **Dense** | Fully Connected | (None, 512) | 1,311,232 |\n| **Dense_1** | Fully Connected | (None, 15) | 7,695 |\n\n---\n\n## **Total Parameters:** 65,416,614 (≈ 249.54 MB)  \n- **Trainable Parameters:** 1,318,927 (≈ 5.03 MB)  \n- **Non-trainable Parameters:** 64,097,687 (≈ 244.51 MB)  \n\n","metadata":{}},{"cell_type":"markdown","source":"# 2| Importing libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport random\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nfrom PIL import Image\n\nimport seaborn as sns\nimport matplotlib.image as img\nimport matplotlib.pyplot as plt\nimport torch\ntorch.manual_seed(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T08:31:40.335778Z","iopub.execute_input":"2025-02-01T08:31:40.336101Z","iopub.status.idle":"2025-02-01T08:31:40.342877Z","shell.execute_reply.started":"2025-02-01T08:31:40.336076Z","shell.execute_reply":"2025-02-01T08:31:40.342009Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3| Getting the path and Loading the data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/human-action-recognition-har-dataset/Human Action Recognition/Training_set.csv\")\ntest_data = pd.read_csv(\"../input/human-action-recognition-har-dataset/Human Action Recognition/Testing_set.csv\")","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:31:40.345854Z","iopub.execute_input":"2025-02-01T08:31:40.346083Z","iopub.status.idle":"2025-02-01T08:31:40.368888Z","shell.execute_reply.started":"2025-02-01T08:31:40.346062Z","shell.execute_reply":"2025-02-01T08:31:40.368199Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:03:52.920312Z","iopub.execute_input":"2025-02-01T09:03:52.920717Z","iopub.status.idle":"2025-02-01T09:03:52.933141Z","shell.execute_reply.started":"2025-02-01T09:03:52.920673Z","shell.execute_reply":"2025-02-01T09:03:52.932179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = train_data.label.unique()\nlabel2idx = {label: idx for idx, label in enumerate(labels)}\nidx2label = {idx: label for idx, label in enumerate(labels)}\nlabel2idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T08:31:40.382866Z","iopub.execute_input":"2025-02-01T08:31:40.383093Z","iopub.status.idle":"2025-02-01T08:31:40.390730Z","shell.execute_reply.started":"2025-02-01T08:31:40.383073Z","shell.execute_reply":"2025-02-01T08:31:40.389897Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### skip test because it does not have ground truth values","metadata":{}},{"cell_type":"code","source":"train_data.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:31:40.395389Z","iopub.execute_input":"2025-02-01T08:31:40.396082Z","iopub.status.idle":"2025-02-01T08:31:40.407434Z","shell.execute_reply.started":"2025-02-01T08:31:40.396052Z","shell.execute_reply":"2025-02-01T08:31:40.406604Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nfrom typing import List, Tuple\n\n# -----------------------------\n# Preprocessing functions\n# -----------------------------\ndef create_preprocessor(input_size: Tuple[int, int],\n                        mean: List[float] = (0.485, 0.456, 0.406),\n                        std: List[float] = (0.229, 0.224, 0.225)):\n    \"\"\"\n    Basic preprocessing: Resize, convert to tensor, and normalize.\n    \"\"\"\n    return transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mean, std=std),\n        # Note: The following Lambda unsqueeze is optional.\n        # DataLoaders already add the batch dimension, so it might not be needed.\n        # transforms.Lambda(lambda x: x.unsqueeze(0))\n    ])\n\ndef create_train_augmentations(input_size: Tuple[int, int],\n                               mean: List[float] = (0.485, 0.456, 0.406),\n                               std: List[float] = (0.229, 0.224, 0.225)):\n    \"\"\"\n    Preprocessing for training that includes augmentations:\n      - RandomResizedCrop: scales and crops the image randomly.\n      - RandomHorizontalFlip: randomly flips the image.\n      - ColorJitter: applies random photometric distortions.\n      - Finally converts to tensor and normalizes.\n    \"\"\"\n    return transforms.Compose([\n        transforms.RandomResizedCrop(input_size, scale=(0.8, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mean, std=std)\n    ])\n\n# -----------------------------\n# Custom Dataset Class\n# -----------------------------\nclass ActionDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, root_dir: str, transform=None, label2idx=label2idx):\n        \"\"\"\n        Args:\n            df (pd.DataFrame): DataFrame with columns \"filename\" and \"label\".\n            root_dir (str): Directory where the images are stored.\n            transform: Transformations to apply to each image.\n        \"\"\"\n        self.df = df.reset_index(drop=True)\n        self.root_dir = root_dir\n        self.transform = transform\n\n        self.label2idx = label2idx\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # Get the row from the DataFrame.\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.root_dir, row['filename'])\n\n        # Open the image and ensure it is in RGB format.\n        image = Image.open(img_path).convert('RGB')\n\n        # Convert the textual label into an integer.\n        label = self.label2idx[row['label']]\n\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# -----------------------------\n# Main setup: Split DataFrames and create DataLoaders\n# -----------------------------\n\n# Split train_df into training and validation sets (80% train, 20% val)\ntrain_split_df, val_split_df = train_test_split(\n    train_data, \n    test_size=0.2, \n    stratify=train_data['label'], \n    random_state=42\n)\n\n# Define the input size expected by your model (e.g., Sapiens expects 1024x1024)\ninput_size = (1024, 1024)\n\n# Create transformation pipelines.\ntrain_transform = create_train_augmentations(input_size)\nval_transform = create_preprocessor(input_size)\n\n# Set the directory where your images are stored.\ntrain_data_root_dir = \"/kaggle/input/human-action-recognition-har-dataset/Human Action Recognition/train\"  # Replace with your actual image directory.\ntest_data_root_dir = \"/kaggle/input/human-action-recognition-har-dataset/Human Action Recognition/test\"\n# Create datasets.\ntrain_dataset = ActionDataset(train_split_df, train_data_root_dir, transform=train_transform)\nval_dataset = ActionDataset(val_split_df, train_data_root_dir, transform=val_transform)\n# test_dataset = ActionDataset(test_data, test_data_root_dir, transform=val_transform)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T08:31:40.420749Z","iopub.execute_input":"2025-02-01T08:31:40.421342Z","iopub.status.idle":"2025-02-01T08:31:40.934174Z","shell.execute_reply.started":"2025-02-01T08:31:40.421321Z","shell.execute_reply":"2025-02-01T08:31:40.933495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# finetuning SAPIENS\n","metadata":{}},{"cell_type":"code","source":"# # !wget https://huggingface.co/facebook/sapiens-pretrain-0.3b/resolve/main/sapiens_0.3b_epoch_1600_clean.pth\n!wget https://huggingface.co/facebook/sapiens-pretrain-0.3b-torchscript/resolve/main/sapiens_0.3b_epoch_1600_torchscript.pt2\n\n!wget -nc https://learnopencv.com/wp-content/uploads/2024/09/man-horse-arrow-scaled.jpg -O man-horse-arrow.jpg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T08:31:40.935321Z","iopub.execute_input":"2025-02-01T08:31:40.935585Z","iopub.status.idle":"2025-02-01T08:32:15.411973Z","shell.execute_reply.started":"2025-02-01T08:31:40.935544Z","shell.execute_reply":"2025-02-01T08:32:15.410640Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch import nn\nclass ImageActionClassifier(nn.Module):\n    def __init__(self, sapiens_model, num_classes):\n        super(ImageActionClassifier, self).__init__()\n        self.sapiens_model = sapiens_model\n        for param in self.sapiens_model.parameters():\n            param.requires_grad = False\n    \n        self.classifier_head = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(1024, num_classes)\n        )\n\n\n    def forward(self, x):\n        with torch.no_grad():\n            x = self.sapiens_model(x)\n        return self.classifier_head(x[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T08:50:38.898735Z","iopub.execute_input":"2025-02-01T08:50:38.899624Z","iopub.status.idle":"2025-02-01T08:50:38.905242Z","shell.execute_reply.started":"2025-02-01T08:50:38.899586Z","shell.execute_reply":"2025-02-01T08:50:38.904314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sapiens_model_path = \"sapiens_0.3b_epoch_1600_torchscript.pt2\"\nsapiens_model = torch.jit.load(sapiens_model_path)\nsapiens_model.eval()\n\n\nmodel = ImageActionClassifier(sapiens_model, 15)\n# model(torch.rand(size=(1,3,1024, 1024)))\n\ncriterion = nn.CrossEntropyLoss()\noptim = torch.optim.AdamW(model.parameters(), lr = 1e-3)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel.\")\n    model = torch.nn.DataParallel(model)\nmodel = model.to(device)\nprint(\"device \", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T08:50:42.042152Z","iopub.execute_input":"2025-02-01T08:50:42.043104Z","iopub.status.idle":"2025-02-01T08:50:43.297037Z","shell.execute_reply.started":"2025-02-01T08:50:42.043068Z","shell.execute_reply":"2025-02-01T08:50:43.296183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_parameters = sum(p.numel() for p in model.parameters() )\nprint(f\"Number of parameters: {num_parameters}\")\nnum_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Number of trainable parameters: {num_parameters}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T08:32:15.486100Z","iopub.status.idle":"2025-02-01T08:32:15.486401Z","shell.execute_reply.started":"2025-02-01T08:32:15.486236Z","shell.execute_reply":"2025-02-01T08:32:15.486250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\n\napi_key = \"330a4a0723c3988c8d367cbb822d3d6624621fbd\"\nwandb.login(key=api_key)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:27:16.922355Z","iopub.execute_input":"2025-02-01T09:27:16.922704Z","iopub.status.idle":"2025-02-01T09:27:20.242014Z","shell.execute_reply.started":"2025-02-01T09:27:16.922676Z","shell.execute_reply":"2025-02-01T09:27:20.241305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 96\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:33:53.350122Z","iopub.execute_input":"2025-02-01T09:33:53.350468Z","iopub.status.idle":"2025-02-01T09:33:53.355145Z","shell.execute_reply.started":"2025-02-01T09:33:53.350438Z","shell.execute_reply":"2025-02-01T09:33:53.354227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nimport wandb\nfrom pathlib import Path\n\nname = \"sapiens-backbone-image-action-epoch4-lre-3run2\"\nwandb.init(project=\"HAR\", name=f\"{name}\")\nPath(\"./checkpoints\").mkdir(parents=True, exist_ok=True)\n\nepochs = 4\nbest_val_loss = float('inf')\nstart = time.time()\nglobal_step = 0  # Add a global step counter\n\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0\n    train_correct = 0\n    train_total = 0\n    \n    # Training loop\n    for batch_idx, batch in enumerate(train_loader):\n        pixel_values = batch[0].to(device)\n        labels = batch[1].to(device)\n        \n        # Forward pass\n        outputs = model(pixel_values)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass\n        optim.zero_grad()\n        loss.backward()\n        \n        # Gradient norm calculation\n        total_norm = 0\n        for param in model.parameters():\n            if param.grad is not None:\n                param_norm = param.grad.data.norm(2)\n                total_norm += param_norm.item() ** 2\n        total_norm = total_norm ** 0.5\n        \n        optim.step()\n        \n        # Compute batch accuracy\n        preds = torch.argmax(outputs, dim=1)\n        batch_correct = (preds == labels).sum().item()\n        batch_total = labels.size(0)\n        batch_accuracy = batch_correct / batch_total\n        \n        # Log every step\n        wandb.log({\n            \"Step\": global_step,\n            \"Step Train Loss\": loss.item(),\n            \"Step Train Accuracy\": batch_accuracy,\n            \"Step Gradient Norm\": total_norm,\n            \"Learning Rate\": optim.param_groups[0]['lr'],\n            \"GPU Memory\": torch.cuda.memory_allocated(device) / (1024 * 1024)\n        })\n        \n        # Accumulate metrics for epoch-level logging\n        train_loss += loss.item() * batch_total\n        train_correct += batch_correct\n        train_total += batch_total\n        \n        global_step += 1\n        \n        if batch_idx % 100 == 0:  # Print every 100 steps\n            print(f\"Epoch {epoch+1}, Step {batch_idx}, \"\n                  f\"Step Loss: {loss.item():.4f}, \"\n                  f\"Step Accuracy: {batch_accuracy*100:.2f}%\")\n        \n    \n    # Calculate epoch-level metrics\n    epoch_train_loss = train_loss / train_total\n    epoch_train_accuracy = train_correct / train_total\n    \n    # Validation step\n    model.eval()\n    val_loss = 0\n    val_correct = 0\n    val_total = 0\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            pixel_values = batch[0].to(device)\n            labels = batch[1].to(device)\n            \n            outputs = model(pixel_values)\n            batch_loss = criterion(outputs, labels).item()\n            \n            preds = torch.argmax(outputs, dim=1)\n            val_correct += (preds == labels).sum().item()\n            val_total += labels.size(0)\n            val_loss += batch_loss * labels.size(0)\n    \n    # Calculate and log epoch-level validation metrics\n    epoch_val_loss = val_loss / val_total\n    epoch_val_accuracy = val_correct / val_total\n    \n    # Log epoch-level metrics\n    wandb.log({\n        \"Epoch\": epoch,\n        \"Epoch Train Loss\": epoch_train_loss,\n        \"Epoch Train Accuracy\": epoch_train_accuracy,\n        \"Epoch Validation Loss\": epoch_val_loss,\n        \"Epoch Validation Accuracy\": epoch_val_accuracy,\n    })\n    \n    # Save best model\n    if epoch_val_loss < best_val_loss:\n        best_val_loss = epoch_val_loss\n        save_path = \"./checkpoints\"\n        torch.save(\n            model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict(), \n            f\"{save_path}/model.pt\"\n        )\n        print(f\"Saved checkpoint at {save_path}\")\n    \n    print(f\"Epoch {epoch+1}/{epochs}, \"\n          f\"Train Loss: {epoch_train_loss:.4f}, \"\n          f\"Train Accuracy: {epoch_train_accuracy*100:.2f}%, \"\n          f\"Val Loss: {epoch_val_loss:.4f}, \"\n          f\"Val Accuracy: {epoch_val_accuracy*100:.2f}%\")\n    \n\nwandb.finish()\ndf = pd.DataFrame({\"time taken\": (time.time() - start)/60, \"epochs\": epochs}, index=[0])\ndf.to_csv(\"./time.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:30:20.297539Z","iopub.execute_input":"2025-02-01T09:30:20.298225Z","iopub.status.idle":"2025-02-01T09:31:37.878434Z","shell.execute_reply.started":"2025-02-01T09:30:20.298196Z","shell.execute_reply":"2025-02-01T09:31:37.876948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:19:14.208276Z","iopub.execute_input":"2025-02-01T09:19:14.208932Z","iopub.status.idle":"2025-02-01T09:19:14.219859Z","shell.execute_reply.started":"2025-02-01T09:19:14.208901Z","shell.execute_reply":"2025-02-01T09:19:14.219007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import pandas as pd\n# from collections import defaultdict\n\n# # Ensure the model is in evaluation mode\n# model.eval()\n\n# all_preds = []\n# all_labels = []\n\n# # Evaluate the model on the test set.\n# with torch.no_grad():\n#     for batch in test_loader:\n#         pixel_values = batch[0].to(device)\n#         labels = batch[1].to(device)\n\n#         outputs = model(pixel_values)\n#         preds = torch.argmax(outputs, dim=1)\n\n#         all_preds.append(preds.cpu())\n#         all_labels.append(labels.cpu())\n#         break\n\n# # Concatenate all predictions and labels from the test set\n# all_preds = torch.cat(all_preds)\n# all_labels = torch.cat(all_labels)\n\n# # Compute total accuracy.\n# total_accuracy = (all_preds == all_labels).float().mean().item()\n\n# # Determine the number of classes.\n# # Here, we assume your test dataset (or training dataset) has an attribute 'labels'\n# # that is a sorted list of class names.\n# num_classes = len(test_dataset.labels)  # Adjust if your dataset stores this differently\n\n# # Initialize counters for per-class accuracy.\n# class_correct = defaultdict(int)\n# class_total = defaultdict(int)\n\n# # Compute per-class correct predictions.\n# for true_label, pred in zip(all_labels, all_preds):\n#     true_label = true_label.item()\n#     pred = pred.item()\n#     class_total[true_label] += 1\n#     if true_label == pred:\n#         class_correct[true_label] += 1\n\n# # Prepare data for CSV output.\n# results = []\n# for cls in range(num_classes):\n#     # Get the class name from the dataset; if you don't have names, you can simply use the class index.\n#     class_name = test_dataset.labels[cls] if hasattr(test_dataset, 'labels') else str(cls)\n#     total = class_total[cls]\n#     accuracy = class_correct[cls] / total if total > 0 else 0\n#     results.append({\n#         'Class': class_name,\n#         'Total Samples': total,\n#         'Accuracy': accuracy\n#     })\n\n# # Add an overall accuracy row.\n# results.append({\n#     'Class': 'Overall',\n#     'Total Samples': len(all_labels),\n#     'Accuracy': total_accuracy\n# })\n\n# # Create a DataFrame and save it to CSV.\n# df_results = pd.DataFrame(results)\n# csv_output_path = 'test_accuracy.csv'\n# df_results.to_csv(csv_output_path, index=False)\n\n# print(\"Test results saved to\", csv_output_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-01T09:00:27.664897Z","iopub.execute_input":"2025-02-01T09:00:27.665776Z","iopub.status.idle":"2025-02-01T09:00:27.988788Z","shell.execute_reply.started":"2025-02-01T09:00:27.665737Z","shell.execute_reply":"2025-02-01T09:00:27.987119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4| Making function that take random path and display the image","metadata":{}},{"cell_type":"code","source":"# def displaying_random_images():\n#     num = random.randint(1,10000)\n#     imgg = \"Image_{}.jpg\".format(num)\n#     train = \"../input/human-action-recognition-har-dataset/Human Action Recognition/train/\"\n#     if os.path.exists(train+imgg):\n#         testImage = img.imread(train+imgg)\n#         plt.imshow(testImage)\n#         plt.title(\"{}\".format(train_data.loc[train_data['filename'] == \"{}\".format(imgg), 'label'].item()))\n\n#     else:\n#         #print(train+img)\n#         print(\"File Path not found \\nSkipping the file!!\")","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.494301Z","iopub.status.idle":"2025-02-01T08:32:15.494636Z","shell.execute_reply.started":"2025-02-01T08:32:15.494454Z","shell.execute_reply":"2025-02-01T08:32:15.494469Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# displaying_random_images()","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.497017Z","iopub.status.idle":"2025-02-01T08:32:15.497350Z","shell.execute_reply.started":"2025-02-01T08:32:15.497189Z","shell.execute_reply":"2025-02-01T08:32:15.497204Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# displaying_random_images()","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.498154Z","iopub.status.idle":"2025-02-01T08:32:15.498422Z","shell.execute_reply.started":"2025-02-01T08:32:15.498289Z","shell.execute_reply":"2025-02-01T08:32:15.498301Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# displaying_random_images()","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.500159Z","iopub.status.idle":"2025-02-01T08:32:15.500585Z","shell.execute_reply.started":"2025-02-01T08:32:15.500357Z","shell.execute_reply":"2025-02-01T08:32:15.500377Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# displaying_random_images()","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.501532Z","iopub.status.idle":"2025-02-01T08:32:15.501935Z","shell.execute_reply.started":"2025-02-01T08:32:15.501733Z","shell.execute_reply":"2025-02-01T08:32:15.501753Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5| Data preprocessing","metadata":{}},{"cell_type":"code","source":"# img_data = []\n# img_label = []\n# length = len(train_fol)\n# for i in (range(len(train_fol)-1)):\n#     t = '../input/human-action-recognition-har-dataset/Human Action Recognition/train/' + filename[i]    \n#     temp_img = Image.open(t)\n#     img_data.append(np.asarray(temp_img.resize((160,160))))\n#     img_label.append(situation[i])","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.504697Z","iopub.status.idle":"2025-02-01T08:32:15.505142Z","shell.execute_reply.started":"2025-02-01T08:32:15.504906Z","shell.execute_reply":"2025-02-01T08:32:15.504927Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# img_shape= (160,160,3)","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.506291Z","iopub.status.idle":"2025-02-01T08:32:15.506721Z","shell.execute_reply.started":"2025-02-01T08:32:15.506494Z","shell.execute_reply":"2025-02-01T08:32:15.506515Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# iii = img_data\n# iii = np.asarray(iii)\n# type(iii)","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.507751Z","iopub.status.idle":"2025-02-01T08:32:15.508150Z","shell.execute_reply.started":"2025-02-01T08:32:15.507937Z","shell.execute_reply":"2025-02-01T08:32:15.507957Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# y_train = to_categorical(np.asarray(train_data[\"label\"].factorize()[0]))\n# print(y_train[0])","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.509943Z","iopub.status.idle":"2025-02-01T08:32:15.510349Z","shell.execute_reply.started":"2025-02-01T08:32:15.510141Z","shell.execute_reply":"2025-02-01T08:32:15.510161Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6| Make an CNN model","metadata":{}},{"cell_type":"code","source":"# efficientnet_model = Sequential()\n\n# model = tf.keras.applications.EfficientNetB7(include_top=False,\n#                                             input_shape=(160,160,3),\n#                                             pooling =\"avg\",classes=15,\n#                                              weights=\"imagenet\")\n\n# for layer in model.layers:\n#     layer.trainable=False\n    \n\n# efficientnet_model.add(model)\n# efficientnet_model.add(Flatten())\n# efficientnet_model.add(Dense(512,activation=\"relu\"))\n# efficientnet_model.add(Dense(15,activation=\"softmax\"))","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.512204Z","iopub.status.idle":"2025-02-01T08:32:15.512631Z","shell.execute_reply.started":"2025-02-01T08:32:15.512393Z","shell.execute_reply":"2025-02-01T08:32:15.512412Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# efficientnet_model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.514286Z","iopub.status.idle":"2025-02-01T08:32:15.514697Z","shell.execute_reply.started":"2025-02-01T08:32:15.514473Z","shell.execute_reply":"2025-02-01T08:32:15.514493Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# efficientnet_model.summary()","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.516083Z","iopub.status.idle":"2025-02-01T08:32:15.516509Z","shell.execute_reply.started":"2025-02-01T08:32:15.516275Z","shell.execute_reply":"2025-02-01T08:32:15.516295Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# history = efficientnet_model.fit(iii,y_train,epochs=40)","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.520008Z","iopub.status.idle":"2025-02-01T08:32:15.520319Z","shell.execute_reply.started":"2025-02-01T08:32:15.520173Z","shell.execute_reply":"2025-02-01T08:32:15.520188Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# losses = history.history[\"loss\"]\n# plt.plot(losses)","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.521210Z","iopub.status.idle":"2025-02-01T08:32:15.521505Z","shell.execute_reply.started":"2025-02-01T08:32:15.521368Z","shell.execute_reply":"2025-02-01T08:32:15.521382Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# acc = history.history['accuracy']\n# plt.plot(acc)","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.522702Z","iopub.status.idle":"2025-02-01T08:32:15.523004Z","shell.execute_reply.started":"2025-02-01T08:32:15.522862Z","shell.execute_reply":"2025-02-01T08:32:15.522876Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7| Model predictions","metadata":{}},{"cell_type":"code","source":"# def read_img(fn):\n#     img = Image.open(fn)\n#     return np.asarray(img.resize((160,160)))","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.524461Z","iopub.status.idle":"2025-02-01T08:32:15.524871Z","shell.execute_reply.started":"2025-02-01T08:32:15.524667Z","shell.execute_reply":"2025-02-01T08:32:15.524686Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def test_predict(test_image):\n#     result = efficientnet_model.predict(np.asarray([read_img(test_image)]))\n\n#     itemindex = np.where(result==np.max(result))\n#     prediction = itemindex[1][0]\n#     print(\"probability: \"+str(np.max(result)*100) + \"%\\nPredicted class : \", prediction)\n\n#     image = img.imread(test_image)\n#     plt.imshow(image)\n#     plt.title(prediction)","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.526124Z","iopub.status.idle":"2025-02-01T08:32:15.526532Z","shell.execute_reply.started":"2025-02-01T08:32:15.526314Z","shell.execute_reply":"2025-02-01T08:32:15.526333Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_predict(\"/kaggle/input/human-action-recognition-har-dataset/Human Action Recognition/test/Image_1001.jpg\")","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.527656Z","iopub.status.idle":"2025-02-01T08:32:15.528061Z","shell.execute_reply.started":"2025-02-01T08:32:15.527842Z","shell.execute_reply":"2025-02-01T08:32:15.527862Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_predict(\"/kaggle/input/human-action-recognition-har-dataset/Human Action Recognition/test/Image_101.jpg\")","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.529519Z","iopub.status.idle":"2025-02-01T08:32:15.529852Z","shell.execute_reply.started":"2025-02-01T08:32:15.529704Z","shell.execute_reply":"2025-02-01T08:32:15.529719Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_predict(\"/kaggle/input/human-action-recognition-har-dataset/Human Action Recognition/test/Image_1056.jpg\")","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.531246Z","iopub.status.idle":"2025-02-01T08:32:15.531657Z","shell.execute_reply.started":"2025-02-01T08:32:15.531435Z","shell.execute_reply":"2025-02-01T08:32:15.531455Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_predict(\"/kaggle/input/human-action-recognition-har-dataset/Human Action Recognition/test/Image_1024.jpg\")","metadata":{"execution":{"iopub.status.busy":"2025-02-01T08:32:15.533595Z","iopub.status.idle":"2025-02-01T08:32:15.534035Z","shell.execute_reply.started":"2025-02-01T08:32:15.533793Z","shell.execute_reply":"2025-02-01T08:32:15.533814Z"},"trusted":true},"outputs":[],"execution_count":null}]}